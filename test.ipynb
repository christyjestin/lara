{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from itertools import chain\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "\n",
    "from models import YNet\n",
    "from factor_clique import FactorClique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YNet(\n",
       "  (left_pipe): MLP(\n",
       "    (network): Sequential(\n",
       "      (0): Linear(in_features=32, out_features=256, bias=True)\n",
       "      (1): ELU(alpha=1.0)\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (3): ELU(alpha=1.0)\n",
       "      (4): Linear(in_features=256, out_features=32, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (right_pipe): MLP(\n",
       "    (network): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "      (1): ELU(alpha=1.0)\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (3): ELU(alpha=1.0)\n",
       "      (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (out_pipe): MLP(\n",
       "    (network): Sequential(\n",
       "      (0): Linear(in_features=96, out_features=256, bias=True)\n",
       "      (1): ELU(alpha=1.0)\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (3): ELU(alpha=1.0)\n",
       "      (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INT_DIM = 32\n",
    "RULE_DIM = 64\n",
    "HIDDEN_DIM = 256\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ynet = YNet([INT_DIM, HIDDEN_DIM, HIDDEN_DIM, INT_DIM], \n",
    "            [RULE_DIM, HIDDEN_DIM, HIDDEN_DIM, RULE_DIM], \n",
    "            [INT_DIM + RULE_DIM, HIDDEN_DIM, HIDDEN_DIM, 1], \n",
    "            nn.ELU,\n",
    "            )\n",
    "ynet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:42<00:00, 23.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# clique parameters\n",
    "min_factor = 11\n",
    "max_factor = 50\n",
    "max_val = 1000\n",
    "\n",
    "# num sample parameters\n",
    "num_cliques = 10\n",
    "num_examples = 5\n",
    "num_samples = 10\n",
    "\n",
    "# training parameters\n",
    "num_inner_iters = 20\n",
    "num_outer_iters = 100000\n",
    "inner_lr = 1e-2\n",
    "outer_lr = 1e-3\n",
    "\n",
    "# used for inner loop\n",
    "max_score = 2\n",
    "\n",
    "losses = []\n",
    "optimizer = optim.Adam(ynet.parameters(), lr=outer_lr)\n",
    "for _ in trange(num_outer_iters):\n",
    "    factors = [random.randint(11, max_factor) for _ in range(num_cliques)]\n",
    "    cliques = [FactorClique(factor, max_val) for factor in factors]\n",
    "    examples = [clique.generate_examples(num_examples) for clique in cliques]\n",
    "\n",
    "    # you can't start these values at 0 because then you'll just copy the first sample over and over\n",
    "    # again: this setup will start with prob of 0.5 but will approach the ideal prob over time\n",
    "    numerator_fill = (num_examples + num_samples) * max_score / 2\n",
    "    denominator_fill = (num_examples + num_samples) * max_score\n",
    "    rule_numerator = torch.full((num_cliques, RULE_DIM), numerator_fill, dtype=torch.float)\n",
    "    rule_denominator = torch.full((num_cliques, RULE_DIM), denominator_fill, dtype=torch.float)\n",
    "    target = torch.tile(torch.cat((torch.ones(num_examples), torch.zeros(num_samples))), (num_cliques,))\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    unreduced_loss_fn = nn.BCEWithLogitsLoss(reduction='none')\n",
    "    # no backprop during inner loop\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_inner_iters):\n",
    "            samples = [clique.generate_samples(num_samples) for clique in cliques]\n",
    "            x = cliques[0].encode_samples(list(chain(*[a + b for a, b in zip(examples, samples)])))\n",
    "            rule_sample = torch.bernoulli(rule_numerator / rule_denominator)\n",
    "            y = torch.repeat_interleave(rule_sample, num_examples + num_samples, dim=0)\n",
    "            x.to(device), y.to(device)\n",
    "            preds = ynet(x, y).squeeze()\n",
    "            loss = unreduced_loss_fn(preds, target)\n",
    "            # flip and shift so that better results mean higher scores\n",
    "            score = torch.clamp(max_score - loss, min=0)\n",
    "            score = score.reshape((-1, num_cliques)).sum(dim=0).unsqueeze(-1)\n",
    "            rule_numerator += score * rule_sample\n",
    "            rule_denominator += score\n",
    "\n",
    "    # in the outer loop, we're at test time, so we use ground truth counterexamples instead of random samples\n",
    "    counter = [clique.generate_counterexamples(num_samples) for clique in cliques]\n",
    "    x = cliques[0].encode_samples(list(chain(*[a + b for a, b in zip(examples, counter)])))\n",
    "    rule_sample = torch.bernoulli(rule_numerator / rule_denominator)\n",
    "    y = torch.repeat_interleave(rule_sample, num_examples + num_samples, dim=0)\n",
    "    x.to(device), y.to(device)\n",
    "    preds = ynet(x, y).squeeze()\n",
    "    loss = loss_fn(preds, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    losses.append(loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
